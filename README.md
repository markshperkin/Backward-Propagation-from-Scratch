# Backward Propagation from Scratch

This repository contains implementations of both **one-layer** and **two-layer** neural networks, built from scratch using **NumPy**. 
These networks are designed for classification and regression tasks and utilize activation functions such as **ReLU** and **Sigmoid**. 
The primary purpose is to demonstrate how neural networks can be constructed without relying on external deep-learning libraries.

## How to Run

### Prerequisites
Make sure you have **Python** and **NumPy** installed on your system.

### Steps to Run
1. Clone the repository to your local machine:
   ```bash
   git clone https://github.com/markshperkin/Backward-Propagation-from-Scratch.git

2. Navigate into the project directory
   ```bash
   cd Backward-Propagation-from-Scratch
   ```
2. Run the program
   ```bash
   python .\OneLayerNN.py
   ```
   ```bash
   python .\TwoLayerNN.py
   ```

## Class Project

This project was developed as part of the Neural Network class under the instruction of [Professor Vignesh Narayanan](https://sc.edu/study/colleges_schools/engineering_and_computing/faculty-staff/narayanan_vignesh.php) at the University of South Carolina.


