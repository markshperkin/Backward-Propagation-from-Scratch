# Backward Propagation from Scratch

This repository contains implementations of both **one-layer** and **two-layer** neural networks, built from scratch using **NumPy**. 
These networks are designed for classification and regression tasks and utilize activation functions such as **ReLU** and **Sigmoid**. 
The primary purpose is to demonstrate how neural networks can be constructed without relying on external deep learning libraries.

## How to Run

### Prerequisites
Make sure you have **Python** and **NumPy** installed on your system.

### Steps to Run
1. Clone the repository to your local machine:
   ```bash
   git clone https://github.com/markshperkin/Backward-Propagation-from-Scratch
.git
   cd your-repo
   ```
2. Run the program
   ```bash
   python .\OneLayerNN.py
   ```
   ```bash
   python .\TwoLayerNN.py
   ```

